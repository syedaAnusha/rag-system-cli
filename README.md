# RAG CLI System

A Retrieval-Augmented Generation (RAG) Command Line Interface system built using LangChain and Google's Generative AI (Gemini).

## Project Overview

This CLI system implements a RAG pipeline that enhances Large Language Model (LLM) responses by retrieving relevant context from a document collection. It uses Google's Gemini model for both text generation and embeddings.

## Features

- üìö Document Processing
  - PDF document support with efficient chunking
  - RecursiveCharacterTextSplitter with configurable parameters
  - Metadata preservation (page numbers, chunk numbers)
- üîç Advanced Search & Retrieval
  - FAISS vector store for efficient similarity search
  - Google's Gemini model for embeddings
  - Configurable context window (k parameter)
  - Multiple retrieval techniques:
    - Similarity Search: Basic retrieval with stuff chain
    - MMR (Maximum Marginal Relevance): Diverse results with contextual compression
    - Multiple-Query: Comprehensive results with query expansion and cross-encoder reranking
  - Advanced reranking:
    - Cross-encoder document reranking using ms-marco-MiniLM for optimized relevance
    - Automatic document ordering based on cross-encoder relevance scores - 2D UMAP Visualization of semantic space:
    - Shows document corpus as background
    - Displays reranked documents with distance scores and color gradient
    - Shows original query (Q) with connecting lines to retrieved documents
    - Accurate distance preservation between query and retrieved documents
- üí° Question Answering
  - Multiple chain types optimized for different scenarios:
    - ConversationalRetrievalChain with stuff chain for chat sessions
    - RetrievalQA with refine chain for one-off MMR queries
    - Multiple-query expansion for comprehensive answers
  - Contextual answer generation
  - Source attribution with page and chunk references
  - Rich formatted output with code highlighting

## Tech Stack

- **LLM**: Google Gemini (gemini-2.0-flash)
- **Embeddings**: Google Generative AI Embeddings (models/embedding-001)
- **Framework**: LangChain
- **Vector Store**: FAISS
- **Document Processing**: Support for PDF, DOCX, and other text formats

## Project Structure

```
rag-system-cli/
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ config/            # Configuration settings
‚îÇ   ‚îú‚îÄ‚îÄ core/             # Core RAG components
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ llm.py        # LLM configuration
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ embeddings.py # Embedding model setup
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ vectorstore.py# Vector store operations
‚îÇ   ‚îú‚îÄ‚îÄ document_processing/ # Document handling
‚îÇ   ‚îî‚îÄ‚îÄ utils/            # Utility functions
‚îú‚îÄ‚îÄ data/
‚îÇ   ‚îú‚îÄ‚îÄ processed/        # Processed document storage
‚îÇ   ‚îî‚îÄ‚îÄ raw/             # Raw document storage
‚îú‚îÄ‚îÄ main.py              # Entry point
‚îî‚îÄ‚îÄ requirements.txt     # Project dependencies
```

## Setup and Installation

1. Clone the repository
2. Ensure Python 3.8+ is installed
3. Create and activate a virtual environment:
   ```bash
   python -m venv venv
   .\venv\Scripts\activate  # Windows
   ```
4. Install dependencies:
   ```bash
   pip install -r requirements.txt
   ```

## Usage

### 1. Index a Document

Index a PDF document to make it searchable:

```bash
python main.py index "documents/your-document.pdf"
```

### 2. Search and Ask Questions

Query the indexed document with questions:

```powershell
python main.py search "your question here"
```

Options:

- `--k`: Number of chunks to use for context (default: 4)

  ```powershell
  python main.py search "what is closure?" --k 5
  ```

- `--search-type`: Retrieval technique to use (default: similarity)

  - `similarity`: Basic similarity search with stuff chain
  - `mmr`: MMR with contextual compression for diverse results
  - `multiple-query`: Comprehensive search using query expansion ```powershell

  ### Using MMR with contextual compression

  python main.py search "what is useState hook?" --search-type mmr --k 5

  ### Using multiple-query expansion (generates UMAP visualization)

  python main.py search "what is useState hook?" --search-type multiple-query --k 6 --num-queries 3

  ### Using multiple-query expansion with cross-encoder reranking

  python main.py search "what is useState hook?" --search-type multiple-query --k 6 --num-queries 3

  ### The multiple-query mode now includes:

  1. Query expansion using LLM

  2. Document retrieval for each expanded query

  3. Cross-encoder reranking for optimal document relevance

  4. UMAP visualization showing semantic space

The multiple-query expansion mode generates a UMAP visualization that shows:

- Document corpus as light gray background points
- Retrieved documents colored by relevance score
- Original query as a red star (Q)
- Dashed lines connecting query to retrieved documents
- Cosine distance scores for each retrieved document

This visualization helps in understanding:

- How the retrieved documents relate to the queries in semantic space
- The distribution of document chunks in the corpus
- The relationship between original and expanded queries

### 3. Interactive Chat Mode

Start an interactive chat session with conversation memory:

```powershell
python main.py chat
```

Options:

- `--k`: Number of chunks to use for context (default: 4)
- `--search-type`: Retrieval technique (default: similarity)
  - `similarity`: Basic search with conversation memory
  - `mmr`: Diverse results while maintaining chat context
  - `multiple-query`: Comprehensive answers with query expansion
- `--num-queries`: Number of expanded queries (for multiple-query mode)
- Chat mode always maintains conversation history

```powershell
# Basic chat with similarity search
python main.py chat --k 4 --search-type similarity

# Chat with MMR for diverse responses
python main.py chat --k 6 --search-type mmr

# Chat with query expansion
python main.py chat --k 6 --search-type multiple-query --num-queries 3
```

The search results include:

- Your question
- A comprehensive answer from the LLM
- Source document information
- Page and chunk references for transparency
- Code snippets with syntax highlighting (when present)

## Contributing

1. Fork the repository
2. Create a feature branch
3. Commit your changes
4. Push to the branch
5. Create a Pull Request

## License

[MIT License](LICENSE)

## Contact

For any questions or feedback, please open an issue in the repository.
